"""This script makes it easier to manually look through genotypes of many samples at known pathogenic loci and
check for samples that may be expanded into the pathogenic range.

The script's main input file is a .tsv file generated by the combine_str_json_to_tsv.py script which
combines multiple ExpansionHunter .json output files into a single .tsv table. It's assumed that this input .tsv will
have at least the columns listed below. One way to add these Sample_* and VariantCatalog_* columns is to run
combine_str_json_to_tsv.py with the --sample-metadata and the --variant-catalog args.

The input .tsv must have at least these input columns:

    "LocusId", "SampleId", "Sample_affected", "Sample_sex",
    "Num Repeats: Allele 1", "Num Repeats: Allele 2", "CI end: Allele 1", "CI end: Allele 2",

"""

import argparse
import os
import subprocess
from ast import literal_eval

import pandas as pd
from tabulate import tabulate


REQUIRED_COLUMNS = [
    "LocusId",
    "SampleId",
    "Sample_affected",
    "Sample_sex",
    "Num Repeats: Allele 1",
    "Num Repeats: Allele 2",
    "CI end: Allele 1",
    "CI end: Allele 2",
]

OPTIONAL_COLUMNS = [
    "RepeatUnit",
    "VariantId",

    "Sample_analysis_status",
    "Sample_coded_phenotype",
    "Sample_phenotypes",
    "Sample_genome_version",
    "Sample_sample_type",

    "VariantCatalog_Inheritance",
    "VariantCatalog_Diseases",

    "Genotype",
    "GenotypeConfidenceInterval",
]

GNOMAD_STR_TABLE_PATH = "https://gnomad-public-us-east-1.s3.amazonaws.com/release/3.1.3/tsv/gnomAD_STR_genotypes__2022_01_20.tsv.gz"


def run(command):
    print(command)
    subprocess.check_call(command, shell=True)


def main():
    p = argparse.ArgumentParser()
    grp = p.add_mutually_exclusive_group(required=True)
    grp.add_argument("--use-affected", action="store_true", help="Use affected status to determine which samples to "
        "include in the output for each locus. Only include those affected samples that have longer expansions than "
        "the top 10 unaffected samples (adjustsable by --max-n-unaffected).")
    grp.add_argument("--use-thresholds", action="store_true", help="Use known pathogenic thresholds to determine which "
        "samples to include in the output for each locus. All samples with expansions above the pathogenic threshold "
        "will be included.")
    p.add_argument("-n", "--max-rows", type=int, default=10000000, help="Limit the max number of samples to include in "
        "the output for each locus.")
    p.add_argument("--max-n-unaffected", type=int, default=10, help="After this many unaffected samples are "
        "encountered with genotypes above a particular expansion size at locus, all samples (affected or unaffected) "
        "that have smaller expansions at that locus will be ignored")
    p.add_argument("--use-gnomad", action="store_true", help="Include samples from the gnomAD v3.1 STR release"
        "@ https://gnomad.broadinstitute.org/downloads#v3-short-tandem-repeats")
    p.add_argument("-l", "--locus", action="append", help="If specified, only these locus ids will be processed")
    p.add_argument("--highlight-samples", nargs="*", help="If specified, this can be the path of a text file that "
        "contains sample ids (one per line) or just 1 or more sample ids listed on the commandline - one per line")
    p.add_argument("combined_tsv_path", nargs="+", help="Path of combined ExpansionHunter .tsv table generated by the "
        "combine_str_json_to_tsv.py script. It's assumed that combine_str_json_to_tsv.py "
        "was run with --sample-metadata and --variant-catalog args to add sample-level and locus-level metadata columns")

    # Parse and validate command-line args + read in the combined table(s) from the given command_tsv_path(s)
    args = p.parse_args()

    if args.use_gnomad:
        print("Loading gnomAD STR table...")
        gnomad_df = pd.read_table(GNOMAD_STR_TABLE_PATH, usecols=[
                "Id",
                "LocusId",
                "ReferenceRegion",
                "Motif",
                "IsAdjacentRepeat",
                "Sex",
                "Allele1",
                "Allele2",
                "Genotype",
                "GenotypeConfidenceInterval",
            ],
            dtype={
                "Id": str,
                "Sex": str,
                "Allele1": str,
                "Allele2": str,
                "GenotypeConfidenceInterval": str,
                "IsAdjacentRepeat": bool,
            },
        )
        gnomad_df = gnomad_df[~gnomad_df["IsAdjacentRepeat"]]
        print(f"Processing {len(gnomad_df)} gnomAD table rows...")
        for column_name, value in [
            ("Sample_genome_version", "38"),
            ("Sample_sample_type", "WGS"),
            ("Sample_affected", "Not Affected"),
        ]:
            gnomad_df.loc[:, column_name] = value

        gnomad_df.loc[:, "Id"] = "gnomAD:" + gnomad_df["Id"]
        gnomad_df.loc[:, "VariantId"] = gnomad_df["LocusId"]

        gnomad_df.loc[:, ("CI1", "CI2")] = gnomad_df["GenotypeConfidenceInterval"].str.split("/", expand=True)
        gnomad_df.loc[:, "CI1"] = gnomad_df["CI1"].fillna("")
        gnomad_df.loc[:, "CI2"] = gnomad_df["CI2"].fillna("")
        gnomad_df.loc[:, ("CI start: Allele 1", "CI end: Allele 1")] = gnomad_df["CI1"].str.split("-", expand=True)
        gnomad_df.loc[:, ("CI start: Allele 2", "CI end: Allele 2")] = gnomad_df["CI2"].str.split("-", expand=True)
        gnomad_df.drop(columns=["CI1", "CI2", "CI start: Allele 1", "CI start: Allele 2"], inplace=True)
        gnomad_df.rename({
            "Id": "SampleId",
            "Sex": "Sample_sex",
            "Allele1": "Num Repeats: Allele 1",
            "Allele2": "Num Repeats: Allele 2",
            "Motif": "RepeatUnit",
        }, axis="columns", inplace=True)


    highlight_sample_ids = []
    if args.highlight_samples:
        for h in args.highlight_samples:
            if os.path.isfile(h):
                with open(h, "rt") as f:
                    sample_ids_list = [s.strip() for s in f.readlines()]
                print(f"Read {len(sample_ids_list)} sample ids to highlight from: {h}")
                highlight_sample_ids.extend(sample_ids_list)
            else:
                highlight_sample_ids.append(h)

        print(f"Will highlight {len(highlight_sample_ids)} sample ids: {highlight_sample_ids}")
        
    all_dfs = []
    all_locus_ids = set()
    for combined_tsv_path in args.combined_tsv_path:
        if not os.path.isfile(combined_tsv_path):
            p.error(f"{combined_tsv_path} not found")

        df = pd.read_table(combined_tsv_path, low_memory=False)

        # check that all required columns are present
        missing_required_columns = set(REQUIRED_COLUMNS) - set(df.columns)
        if missing_required_columns:
            p.error(f"{combined_tsv_path} is missing these required columns: {missing_required_columns}")

        # fill in values for missing optional columns
        missing_optional_columns = set(OPTIONAL_COLUMNS) - set(df.columns)
        if missing_optional_columns:
            print(f"WARNING: {combined_tsv_path} is missing these columns: {missing_optional_columns}. "
                  f"Filling them with None...")
            for c in missing_optional_columns:
                df.loc[:, c] = None

        all_locus_ids |= set(df.LocusId)

        if args.use_gnomad:
            print("gnomad_df columns: ", gnomad_df.columns)
            print("df columns: ", df.columns)
            df = pd.concat([df, gnomad_df])
            df.fillna("", inplace=True)

        for integer_column in ("CI end: Allele 1", "CI end: Allele 2"):
            df.loc[:, integer_column] = pd.to_numeric(df[integer_column], errors="coerce")

        all_dfs.append((df, combined_tsv_path))

    # Process each locus
    for locus_id in sorted(all_locus_ids):
        if args.locus and locus_id not in args.locus:
            continue

        print(">"*100)  # print a divider
        for i, (full_df, df_source_path) in enumerate(all_dfs):

            print("="*100)  # print a divider
            print(f"** {locus_id} from {df_source_path} **")

            locus_specific_df = full_df[full_df.LocusId == locus_id]
            if len(locus_specific_df) == 0:
                continue

            locus_specific_df = locus_specific_df.sort_values(by=["Num Repeats: Allele 2", "Num Repeats: Allele 1"], ascending=False)

            # get the 1st row and use it to look up metadata values which are the same across all rows for the locus (eg. Inheritance Mode)
            first_row = locus_specific_df.iloc[0].to_dict()
            inheritance_mode = first_row.get("VariantCatalog_Inheritance") or "Unknown"

            disease_info = first_row.get("VariantCatalog_Diseases")
            intermediate_threshold_min = None
            pathogenic_threshold_min = None
            if disease_info and not pd.isna(disease_info):
                try:
                    disease_info = literal_eval(disease_info)
                except Exception as e:
                    raise ValueError(f"Unable to parse {disease_info} as json: {e}")

                try:
                    intermediate_threshold_min = min(int(float(d["NormalMax"]) + 1) for d in disease_info if d.get("NormalMax"))
                except ValueError as e:
                    print(f"WARNING: {locus_id} couldn't parse NormalMax fields from disease_info {disease_info}: {e}")

                try:
                    pathogenic_threshold_min = min(int(float(d["PathogenicMin"])) for d in disease_info if d.get("PathogenicMin"))
                except ValueError as e:
                    print(f"WARNING: {locus_id} couldn't parse PathogenicMin fields from disease_info {disease_info}: {e}")

            reference_region = first_row["ReferenceRegion"]
            genome_version = f"GRCh{first_row['Sample_genome_version']}" if first_row.get('Sample_genome_version') else ""
            motif = first_row.get("RepeatUnit")
            locus_description = f"{locus_id} ({reference_region}: {genome_version})  https://stripy.org/database/{locus_id}"
            print("**Locus**: ", locus_description)
            print("**Disease**: ", str(disease_info))
            print("**Inheritance**: ", inheritance_mode)
            print("**Pathogenic Threshold**: >=", pathogenic_threshold_min, "x", motif)
            print("**Intermediate Threshold**: >=", intermediate_threshold_min, "x", motif)

            locus_specific_df.loc[:, "Sample_sample_type"] = locus_specific_df["Sample_sample_type"].fillna("Unknown")
            locus_specific_df.loc[:, "Sample_sex"] = locus_specific_df["Sample_sex"].fillna("Unknown")
            locus_specific_df.loc[:, "Sample_affected"] = locus_specific_df["Sample_affected"].fillna("Unknown")

            locus_specific_df = locus_specific_df[locus_specific_df["Sample_affected"] != "Unknown"]

            filtered_df_list = []
            if locus_id == "COMP":
                # COMP is a special case where contractions below 5 repeats are also pathogenic
                df_comp = locus_specific_df[
                    (locus_specific_df["CI end: Allele 1"] < 5) |
                    (locus_specific_df["CI end: Allele 2"] < 5)
                ]
                filtered_df_list.append(df_comp)

            if args.use_thresholds:
                threshold = intermediate_threshold_min or pathogenic_threshold_min
            else:
                threshold = 0

            if threshold:
                if inheritance_mode == "XR":
                    df_female = locus_specific_df[
                        (locus_specific_df["Sample_sex"].str.upper().str.startswith("F") | locus_specific_df["Sample_sex"].str.upper().str.startswith("U")) &
                        (locus_specific_df["CI end: Allele 1"]) &
                        (locus_specific_df["CI end: Allele 1"] >= threshold) &
                        (locus_specific_df["CI end: Allele 2"] >= threshold)
                    ].iloc[0:args.max_rows]
                    df_male = locus_specific_df[
                        (locus_specific_df["Sample_sex"].str.upper().str.startswith("M")) &
                        (locus_specific_df["CI end: Allele 1"] >= threshold)
                    ].iloc[0:args.max_rows]
                    filtered_dfs_for_locus_and_sample_type = [df_male, df_female]
                elif inheritance_mode == "AR":
                    filtered_dfs_for_locus_and_sample_type = [
                        locus_specific_df[
                            (locus_specific_df["CI end: Allele 1"] >= threshold) &
                            (locus_specific_df["CI end: Allele 2"] >= threshold)
                        ] .iloc[0:args.max_rows]
                    ]
                else:
                    filtered_dfs_for_locus_and_sample_type = [
                        locus_specific_df[
                            (locus_specific_df["CI end: Allele 1"] >= threshold) |
                            (locus_specific_df["CI end: Allele 2"] >= threshold)
                        ].iloc[0:args.max_rows]
                    ]
                filtered_df_list += filtered_dfs_for_locus_and_sample_type

            if args.use_affected:
                if inheritance_mode == "XR":
                    dfs_to_process = [
                        locus_specific_df[
                            locus_specific_df["Sample_sex"].str.upper().str.startswith("F")
                        ],
                        locus_specific_df[
                            locus_specific_df["Sample_sex"].str.upper().str.startswith("M")
                        ],
                    ]
                else:
                    dfs_to_process = [locus_specific_df]

                for locus_and_sample_type_and_sex_specific_df in dfs_to_process:

                    unaffected_counter = 0
                    idx = 0
                    for affected_status in locus_and_sample_type_and_sex_specific_df["Sample_affected"]:
                        idx += 1
                        if affected_status and ("Not Affected" in affected_status or "Unknown" in affected_status):
                            unaffected_counter += 1

                        if unaffected_counter >= args.max_n_unaffected:
                            break
                    
                    filtered_df_for_locus_and_sample_type_and_sex = locus_and_sample_type_and_sex_specific_df.iloc[:idx]

                    filtered_df_list.append(filtered_df_for_locus_and_sample_type_and_sex)

        for i, filtered_df_for_locus in enumerate(filtered_df_list):
            print(f"Found {len(filtered_df_for_locus)} samples passed filters in table {i+1} out of {len(filtered_df_list)} for locus {locus_id}")

            if len(filtered_df_for_locus) == 0:
                continue

            inheritance_mode = first_row.get("VariantCatalog_Inheritance") or "Unknown"

            for label, threshold in [
                ("Intermediate Threshold", intermediate_threshold_min),
                ("Pathogenic Threshold", pathogenic_threshold_min),
            ]:
                if not threshold:
                    continue
                threshold_record = {
                    "SampleId": f"**{label}**",
                    "LocusId": locus_id,
                    "VariantId": locus_id,
                    "Genotype": f">= {threshold}",
                }

                for column in filtered_df_for_locus.columns:
                    if column.lower().startswith("sample_") or column in ("GenotypeConfidenceInterval",):
                        threshold_record[column] = "---"
                if inheritance_mode == "XR":
                    filtered_df_for_locus = filtered_df_for_locus.append({
                        **threshold_record,
                        "Num Repeats: Allele 1": threshold,
                        "Sample_sex": "male",
                    }, ignore_index=True)
                    filtered_df_for_locus = filtered_df_for_locus.append({
                        **threshold_record,
                        "Num Repeats: Allele 2": threshold,
                        "Sample_sex": "female",
                    }, ignore_index=True)
                else:
                    filtered_df_for_locus = filtered_df_for_locus.append({
                        **threshold_record,
                        "Num Repeats: Allele 2": threshold,
                    }, ignore_index=True)

            filtered_df_for_locus = filtered_df_for_locus.sort_values(by=["Num Repeats: Allele 2", "Num Repeats: Allele 1"], ascending=False)

            # Print the filtered results for this locus
            filtered_df_for_locus = filtered_df_for_locus[[
                "SampleId",
                "LocusId",
                "VariantId",

                "Genotype",
                "GenotypeConfidenceInterval",

                "Sample_sex",
                "Sample_affected",
                "Sample_sample_type",

                "Sample_analysis_status",
                "Sample_coded_phenotype",
                "Sample_genome_version",

                "RepeatUnit",
                "VariantCatalog_Inheritance",
            ]]
            if highlight_sample_ids:
                highlight_sample_ids = set(highlight_sample_ids)
                filtered_df_for_locus.loc[:, "SampleId"] = filtered_df_for_locus["SampleId"].apply(lambda s: (s if s not in highlight_sample_ids else f"==> {s}"))

            # Shorten some column names so more can fit on screen
            filtered_df_for_locus.rename(columns={
                "Sample_affected": "affected",
                "Sample_sex": "sex",
                "Sample_sample_type": "type",
                "GenotypeConfidenceInterval": "GenotypeCI",
            }, inplace=True)

            for column_name in "affected", "sex", "type":
                filtered_df_for_locus.loc[:, column_name] = filtered_df_for_locus[column_name].apply(
                    lambda s: "; ".join(set(s.split("; "))))

            # Print the candidate pathogenic rows for this locus
            print(tabulate(filtered_df_for_locus, headers="keys", tablefmt="psql", showindex=False))


if __name__ == "__main__":
    main()
