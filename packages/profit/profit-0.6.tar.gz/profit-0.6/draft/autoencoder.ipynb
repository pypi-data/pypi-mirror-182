{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Autoencoder for dimensionality reduction (aka \"Manifold learning\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some test data\n",
    "* Here we use numpy and matplotlib\n",
    "* Two examples of 1D submanifolds in $\\mathbb{R}^2$: a straight line and a spiral\n",
    "* Parameterise $v=(x,y)=F(u)$ by input $u=(\\varphi,\\vartheta)$, with 1D dependency $F=F(\\varphi)$ and dummy $\\vartheta$\n",
    "* Add some noise to emulate realistic conditions and avoid flat gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha = 0.3 # spirality\n",
    "noi = 0.3 # noise\n",
    "\n",
    "ph = np.linspace(0, 2*np.pi, 30) # variable that introduces 1D dependency\n",
    "th = np.linspace(0, 2*np.pi, 30) # mock variable, no dependency\n",
    "\n",
    "[PH, TH] = np.meshgrid(ph,th)\n",
    "PH = PH.flatten(); TH = TH.flatten();\n",
    "PH = PH + (np.random.rand(len(PH))-.5) # redistribute a bit\n",
    "TH = TH + (np.random.rand(len(PH))-.5)\n",
    "\n",
    "R = 1.0 + alpha*PH\n",
    "\n",
    "# Curve 1: straight line\n",
    "curve1 = np.array([PH, PH])\n",
    "curve1n = curve1 + noi*(np.random.rand(curve1.shape[0],curve1.shape[1])-.5)\n",
    "\n",
    "# Curve 2: straight line\n",
    "curve2 = np.array([R*np.cos(PH), R*np.sin(PH)])\n",
    "curve2n = curve2 + noi*(np.random.rand(curve2.shape[0],curve2.shape[1])-.5)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(curve1n[0,:], curve1n[1,:], '.', markersize=3)\n",
    "plt.plot(curve1[0,:], curve1[1,:], '.', markersize=3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(curve2n[0,:], curve2n[1,:], '.', markersize=3)\n",
    "plt.plot(curve2[0,:], curve2[1,:], '.', markersize=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Import models, layers and optimizers from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras frontend inside tensorflow > 1.4\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Alternative: standalone Keras with custom backend\n",
    "# import os\n",
    "# os.environ['KERAS_BACKEND'] = 'theano' # 'cntk', 'tensorflow', or 'theano'\n",
    "# from keras import models, layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "* our first example is a linear function\n",
    "* use linear or \"leaky\" relu or activation function (usual relu cuts away negative part)\n",
    "* linear autoencoder corresponds to PCA/SVD/POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional api\n",
    "inputs = keras.layers.Input(shape=(2,)) # 2 input nodes\n",
    "layer1 = keras.layers.Dense(1, activation='linear')(inputs)  # compress to a single node\n",
    "outputs = keras.layers.Dense(2, activation='linear')(layer1) # extend to 2 output nodes\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Alternative: Sequential api (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import plot_model\n",
    "from IPython import display\n",
    "\n",
    "# from https://medium.com/@zhang_yang/how-to-plot-keras-models-493469884fd5\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "display.Image('model.png')\n",
    "\n",
    "# Alternative with standalone keras:\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# display.SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers: Stochastic gradient descent\n",
    "* Adam: Adaptive moment estimation - \"damped ball rolling in potential well\"<br>\n",
    "D.P. Kingma and B. Jimmy. arXiv preprint arXiv:1412.6980 (2014).\n",
    "* RMSprop: stoch. grad. desc. with dynamically adjusting learning rate<br>\n",
    "G. Hinton, lecture slides http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "\n",
    "Source: http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='mse')           # loss function, mae = mean abs. err., mse = mean square err.\n",
    "\n",
    "model.save_weights('model.h5')      # save weights for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "* Get data to the right shape for Keras\n",
    "* Normalise data to be in range (0,1) for all dimensions\n",
    "* Use `train_test_split` from scikit-learn to automatically\n",
    "split data into some training and test (validation) data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to transpose first axis should be index in dataset\n",
    "input_all = np.array([PH, TH]).T \n",
    "output_all = curve1n.T\n",
    "\n",
    "# normalisation, especially if using sigmoid mapping to (0,1) at output\n",
    "input_all = (input_all - np.min(input_all,0))/np.max(input_all - np.min(input_all,0),0)\n",
    "output_all = (output_all - np.min(output_all,0))/np.max(output_all - np.min(output_all,0),0)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "input_train, input_test, output_train, output_test = train_test_split(\n",
    "     input_all, output_all, test_size=0.33, random_state=13)\n",
    "\n",
    "# for tensorboard diagnostics, see https://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras\n",
    "# start tensorboard with tensorboard --logdir path_to_current_dir/tb \n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./tb', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "model.load_weights('model.h5') # load initial weights to start from scratch every time\n",
    "history = model.fit(           # save training history\n",
    "    input_train, output_train, # training input and output\n",
    "    batch_size=16,             # no. of training pairs to process in parallel while optimising\n",
    "    epochs=128,                # no. of cycles over whole training set while optimising\n",
    "    callbacks=[tb_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot diagnostics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval_train = model.predict(input_train)\n",
    "output_eval_test = model.predict(input_test)\n",
    "\n",
    "# plot diagnostics, \n",
    "# see https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "print(history.history.keys())\n",
    "plt.figure()\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.grid(True)\n",
    "plt.title('loss function over training iterations')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(output_train[:,0], output_train[:,1], '.', markersize=3)\n",
    "plt.plot(output_eval_train[:,0], output_eval_train[:,1], '.', markersize=3)\n",
    "plt.title('training results')\n",
    "plt.legend(['reference', 'network'])\n",
    "plt.figure()\n",
    "plt.plot(output_test[:,0], output_test[:,1], '.', markersize=3)\n",
    "plt.plot(output_eval_test[:,0], output_eval_test[:,1], '.', markersize=3)\n",
    "plt.title('validation results')\n",
    "plt.legend(['reference', 'network'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger and nonlinear network\n",
    "* Need more degrees of freedom to handle spiral\n",
    "* Nonlinear activation function to reproduce nonlinear function\n",
    "* Some intermediate layers to reduce jumps in node count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential api\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Dense(16, input_dim=2, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(256, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(16, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(1, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(16, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(256, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(16, activation=tf.nn.tanh))\n",
    "model2.add(keras.layers.Dense(2, activation=tf.nn.sigmoid))\n",
    "\n",
    "model2.compile(optimizer=tf.train.AdamOptimizer(), loss='mse')\n",
    "\n",
    "input_all = np.array([PH, TH]).T \n",
    "output_all = curve2n.T\n",
    "\n",
    "input_all = (input_all - np.min(input_all,0))/np.max(input_all - np.min(input_all,0),0)\n",
    "output_all = (output_all - np.min(output_all,0))/np.max(output_all - np.min(output_all,0),0)\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(\n",
    "     input_all, output_all, test_size = 0.33, random_state = 13)\n",
    "\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./tb', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = model2.fit(input_train, output_train, batch_size = 16, epochs = 128, callbacks=[tb_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval_train = model2.predict(input_train)\n",
    "output_eval_test = model2.predict(input_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.grid(True)\n",
    "plt.title('loss function over training iterations')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(output_train[:,0], output_train[:,1], '.', markersize=3)\n",
    "plt.plot(output_eval_train[:,0], output_eval_train[:,1], '.', markersize=3)\n",
    "plt.title('training results')\n",
    "plt.legend(['reference', 'network'])\n",
    "plt.figure()\n",
    "plt.plot(output_test[:,0], output_test[:,1], '.', markersize=3)\n",
    "plt.plot(output_eval_test[:,0], output_eval_test[:,1], '.', markersize=3)\n",
    "plt.title('validation results')\n",
    "plt.legend(['reference', 'network'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for k in np.linspace(0,1,10):\n",
    "    input_sens1 = np.array([k*np.ones(100), np.linspace(0,1,100)]).T\n",
    "    output_sens1 = model2.predict(input_sens1)\n",
    "    plt.plot(output_sens1[:,0], output_sens1[:,1], '.', markersize=3)\n",
    "    plt.xlim([0,1]); plt.ylim([0,1])\n",
    "plt.title(r'sensitivity on $\\vartheta$ at fixed $\\varphi$')\n",
    "\n",
    "plt.figure()\n",
    "for k in np.linspace(0,1,10):\n",
    "    input_sens2 = np.array([np.linspace(0,1,100),k*np.ones(100)]).T\n",
    "    output_sens2 = model2.predict(input_sens2)\n",
    "    plt.plot(output_sens2[:,0], output_sens2[:,1], '.', markersize=3)\n",
    "    plt.xlim([0,1]); plt.ylim([0,1])\n",
    "plt.title(r'sensitivity on $\\varphi$ at fixed $\\vartheta$')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
