{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also https://i-systems.github.io/teaching/ML/iNotes/15_Autoencoder.html\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and visual inspection\n",
    "First we do some initializations and normalize the data to be in (0, 1) everywhere. Then we plot the projection of the data to a 2D coordinate plane of sigma_0 and f_Si. We notice a distinct banana shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nskip = 1000  # Use only every 1000th data point\n",
    "skip = np.arange(1e6)\n",
    "skip = np.delete(skip, np.arange(0, 1e6, nskip))\n",
    "\n",
    "data = pd.read_table(\n",
    "    #'/mnt/c/Users/chral/Dropbox/ipp/paper_algae/mc_out.dat', \n",
    "    '/Users/ert/Dropbox/ipp/paper_algae/mc_out.dat',\n",
    "    sep='\\s+', skiprows=skip)\n",
    "names = {\n",
    "    'mu_0': 'k_alg_growth_max',\n",
    "    'f_si': 'frac_si_alg_1',\n",
    "    'lambda_S': 'k_att_shade',\n",
    "    'K_light': 'k_light_sm',\n",
    "    'sigma_0': 'k_alg_loss',\n",
    "    'a': 'coeff_d_loss_2'\n",
    "}\n",
    "indata = data[['k_alg_loss', 'frac_si_alg_1']].values\n",
    "indata = (indata - np.min(indata,0))/np.max(indata - np.min(indata,0), 0)\n",
    "\n",
    "px.scatter(x=indata[:,0], y=indata[:,1], labels={'x': 'sigma_0', 'y': 'f_si'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "Linear methods cannot represent such shapes, so we employ an autoencoder for nonlinear subspace identification. scikit-learn provides a simple way to implement this in a multilayer perceptron (MLP) regressor. We split the data into a training and a test set to judge the quality of the final result. Then we do the fit. Here a BFGS (quasi-Newton) optimizer is used, since the network is small enough. For larger networks one would typically use Adam or Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes = (8, 8, 1, 8, 8), \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'lbfgs', \n",
    "                   max_iter = 512, \n",
    "                   tol = 1e-7, \n",
    "                   verbose = True)\n",
    "train, test = model_selection.train_test_split(indata)\n",
    "reg.fit(train, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and plotting\n",
    "\n",
    "Now we evaluate the autoencoder network for the training and the test data and plot the predictions against the original data. In this example the autoencoder performs visually well. What we see is the fit of the identity transform involving a projection on the submanifold spanned by the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval_train = reg.predict(train[:,:2])\n",
    "output_eval_test = reg.predict(test[:,:2])\n",
    "\n",
    "fig = px.scatter(x=train[:,0], y=train[:,1], labels={'x': 'sigma_0', 'y': 'f_si'}, title='Training data')\n",
    "fig.add_scatter(x=output_eval_train[:,0], y=output_eval_train[:,1], mode='markers', name='autoencoder')\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(x=test[:,0], y=test[:,1], labels={'x': 'sigma_0', 'y': 'f_si'}, title='Test data')\n",
    "fig.add_scatter(x=output_eval_test[:,0], y=output_eval_test[:,1], mode='markers', name='autoencoder')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection and back-projection between reduced and full dimension\n",
    "\n",
    "Now we plot the dependency $x_k = \\xi_k(t)$ of the two parameters in full space represented via the single hidden parameter $t$. Due to limitations of scikit-learn we cut the network in half and use the first half as an inverse transform. Later this should be also done in the right half of the network. In more advanced formulations one may also leverage the symmetry to reduce the number of independent weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.n_layers_ = ((reg.n_layers_ - 2)+1) // 2 + 1\n",
    "\n",
    "ae_parm = reg.predict(train)\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=ae_parm, y=train[:,0], \n",
    "    mode='markers', name='sigma_0')\n",
    "fig.add_scatter(x=ae_parm, y=output_eval_train[:,0], \n",
    "    mode='markers', name='sigma_0 from lift')\n",
    "fig.add_scatter(x=ae_parm, y=train[:,1], \n",
    "    mode='markers', name='f_Si')\n",
    "fig.add_scatter(x=ae_parm, y=output_eval_train[:,1], \n",
    "    mode='markers', name='f_Si from lift')\n",
    "fig.update_layout(\n",
    "    xaxis_title = 't (hidden curve parameter)',\n",
    "    yaxis_title = 'sigma_0, f_Si')\n",
    "fig.show()\n",
    "\n",
    "ae_parm = reg.predict(test)\n",
    "fig = go.Figure()\n",
    "fig.add_scatter(x=ae_parm, y=test[:,0], \n",
    "    mode='markers', name='sigma_0')\n",
    "fig.add_scatter(x=ae_parm, y=output_eval_test[:,0], \n",
    "    mode='markers', name='sigma_0 from lift')\n",
    "fig.add_scatter(x=ae_parm, y=test[:,1], \n",
    "    mode='markers', name='f_Si')\n",
    "fig.add_scatter(x=ae_parm, y=output_eval_test[:,1], \n",
    "    mode='markers', name='f_Si from lift')\n",
    "fig.update_layout(\n",
    "    xaxis_title = 't (hidden curve parameter)',\n",
    "    yaxis_title = 'sigma_0, f_Si')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same with TensorFlow in higher dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "Mdim = 5  # Dimension of autoencoder bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indata = data[names.values()].values  # all 6 parameters\n",
    "indata = (indata - np.min(indata,0))/np.max(indata - np.min(indata,0),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(64, input_dim=6, activation=tf.nn.tanh))\n",
    "model.add(keras.layers.Dense(64, activation=tf.nn.tanh))\n",
    "model.add(keras.layers.Dense(Mdim, activation=tf.nn.tanh))\n",
    "model.add(keras.layers.Dense(64, activation=tf.nn.tanh))\n",
    "model.add(keras.layers.Dense(64, activation=tf.nn.tanh))\n",
    "model.add(keras.layers.Dense(6, activation=None))\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(indata)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train, train))\n",
    "train_dataset = dataset.shuffle(10000).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval = model.predict(indata)\n",
    "deviation = (indata-output_eval)\n",
    "n = list(names.keys())\n",
    "fig = go.Figure()\n",
    "for k in range(len(names)):\n",
    "    fig.add_scatter(y=deviation[:,k], mode='markers', name=n[k])\n",
    "fig.update_yaxes(range=[-1, 1])\n",
    "fig.update_traces(marker={'size': 3})\n",
    "fig.update_layout(title='Deviation of data from prediction', \n",
    "    xaxis_title = 'Index',\n",
    "    yaxis_title = 'Normalized deviation of autoencoder')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
